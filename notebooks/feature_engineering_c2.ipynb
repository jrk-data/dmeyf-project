{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T16:56:05.247214Z",
     "start_time": "2025-11-21T16:56:03.868714Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import duckdb\n",
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "import logging\n",
    "import os\n",
    "import src.config as config\n",
    "\n",
    "# Para autenticarse en Google Cloud\n",
    "from google.auth import default as google_auth_default\n",
    "from google.auth.transport.requests import Request as AuthRequest\n"
   ],
   "id": "34167d12b4b5fdfc",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T19:15:59.049515Z",
     "start_time": "2025-11-21T19:15:59.041439Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def _aut_google():\n",
    "    # 1. --- OBTENER TOKEN DE AUTENTICACIÓN ---\n",
    "    try:\n",
    "        credentials, project = google_auth_default()\n",
    "        credentials.refresh(AuthRequest())\n",
    "        gcs_bearer_token = credentials.token\n",
    "        logger.info(\"Token de Google Cloud obtenido exitosamente.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ No se pudo obtener el token de GCS/ADC: {e}\")\n",
    "        raise\n",
    "    return gcs_bearer_token"
   ],
   "id": "2c3f2d8b2f84d4df",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T16:56:05.606382Z",
     "start_time": "2025-11-21T16:56:05.587223Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "# Configuración básica de logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# La URL del archivo en GCS\n",
    "GCS_FILE_PATH = \"gs://joaquinrk_data_bukito3/datasets/competencia_02_crudo.csv.gz\"\n",
    "\n",
    "def load_gcs_to_bigquery_via_duckdb(\n",
    "    project_id: str,\n",
    "    dataset_id: str,\n",
    "    table_id: str = \"c03\",\n",
    "    gcs_file_path: str = config.DATA_PATH,\n",
    "    temp_local_db: str = \":memory:\" # Usa ':memory:' para base de datos en memoria\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Lee un archivo Gzip CSV desde GCS usando DuckDB,\n",
    "    y luego carga el DataFrame resultante en una tabla de BigQuery.\n",
    "\n",
    "    Args:\n",
    "        project_id: ID del proyecto de Google Cloud.\n",
    "        dataset_id: ID del dataset de destino en BQ.\n",
    "        table_id: Nombre de la tabla de destino en BQ (por defecto, 'c02').\n",
    "        gcs_file_path: Ruta completa del archivo GCS.\n",
    "        temp_local_db: Ruta a la base de datos DuckDB. Usa ':memory:' para RAM.\n",
    "    \"\"\"\n",
    "\n",
    "    gcs_bearer_token = _aut_google()\n",
    "    logger.info(f\"Iniciando proceso: GCS ({gcs_file_path}) -> DuckDB -> BigQuery ({dataset_id}.{table_id})\")\n",
    "\n",
    "    # 1. Conectar DuckDB y leer GCS\n",
    "    try:\n",
    "        # Se requiere instalar el módulo httpfs para leer GCS/S3/HTTPs\n",
    "        con = duckdb.connect(database=temp_local_db, read_only=False)\n",
    "        con.sql(\"INSTALL httpfs;\")\n",
    "        con.sql(\"LOAD httpfs;\")\n",
    "        logger.info(\"DuckDB conectado y extensión 'httpfs' cargada.\")\n",
    "\n",
    "\n",
    "        #    --- SOLUCIÓN: CREAR SECRETO GCS CON EL TOKEN ---\n",
    "        # DuckDB utilizará este secreto para todas las peticiones a GCS\n",
    "        con.sql(f\"CREATE SECRET gcs_secret (TYPE GCS, bearer_token '{gcs_bearer_token}');\")\n",
    "        logger.info(\"DuckDB conectado, extensión 'httpfs' cargada y secreto GCS creado.\")\n",
    "\n",
    "\n",
    "        # Ejecutar la consulta para leer el archivo GCS y obtener el resultado como un DataFrame de Pandas\n",
    "        query = f\"SELECT * FROM read_csv_auto('{gcs_file_path}');\"\n",
    "        df_duckdb = con.sql(query).df()\n",
    "\n",
    "        logger.info(f\"✅ Lectura de GCS a DataFrame completada. Filas cargadas: {len(df_duckdb)}\")\n",
    "\n",
    "        # Cerrar la conexión DuckDB\n",
    "        con.close()\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Error durante la lectura con DuckDB: {e}\")\n",
    "        # Asegúrate de que las credenciales de GCS sean válidas para la extensión httpfs\n",
    "        raise\n",
    "\n",
    "    # 2. Cargar DataFrame a BigQuery\n",
    "    try:\n",
    "        client = bigquery.Client(project=project_id)\n",
    "        table_ref = client.dataset(dataset_id).table(table_id)\n",
    "\n",
    "        # Configuración de carga: escribir sobre la tabla si ya existe\n",
    "        job_config = bigquery.LoadJobConfig(\n",
    "            write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
    "        )\n",
    "\n",
    "        # Cargar el DataFrame de Pandas a BigQuery\n",
    "        job = client.load_table_from_dataframe(\n",
    "            df_duckdb, table_ref, job_config=job_config\n",
    "        )\n",
    "\n",
    "        job.result()  # Espera a que el job termine\n",
    "\n",
    "        logger.info(f\"✅ DataFrame cargado exitosamente a BigQuery en la tabla '{table_id}' en el dataset '{dataset_id}'.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Error durante la carga a BigQuery: {e}\")\n",
    "        raise"
   ],
   "id": "d9a489131a6e53db",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T16:26:02.757804Z",
     "start_time": "2025-11-21T16:15:24.252966Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Ejemplo de Uso (Asegúrate de configurar tus variables) ---\n",
    "# **REEMPLAZA ESTOS VALORES CON LOS DE TU ENTORNO**\n",
    "YOUR_PROJECT_ID = config.BQ_PROJECT\n",
    "YOUR_DATASET_ID = config.BQ_DATASET # Cambia por tu dataset\n",
    "YOUR_TABLE_ID = \"c02\" # Nombre de la tabla de destino\n",
    "\n",
    "try:\n",
    "    logger.info(\"Ejecutando carga de datos desde GCS a BigQuery...\")\n",
    "    logger.info(\"tarda unos 10 minutos..\")\n",
    "\n",
    "    # load_gcs_to_bigquery_via_duckdb(\n",
    "    #     project_id=YOUR_PROJECT_ID,\n",
    "    #     dataset_id=YOUR_DATASET_ID,\n",
    "    #     table_id=YOUR_TABLE_ID\n",
    "    # )\n",
    "except Exception as e:\n",
    "    print(f\"La ejecución falló. Asegúrate de tener credenciales válidas y de que la extensión httpfs esté instalada en DuckDB.\")\n",
    "    pass"
   ],
   "id": "c3ba5bc51711e473",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Token de Google Cloud obtenido exitosamente.\n",
      "INFO:__main__:Iniciando proceso: GCS (gs://joaquinrk_data_bukito3/datasets/competencia_02_crudo.csv.gz) -> DuckDB -> BigQuery (dmeyf.c02)\n",
      "INFO:__main__:DuckDB conectado y extensión 'httpfs' cargada.\n",
      "INFO:__main__:DuckDB conectado, extensión 'httpfs' cargada y secreto GCS creado.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0c232dfb7e16469c8de18beb5915788b"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:✅ Lectura de GCS a DataFrame completada. Filas cargadas: 4717958\n",
      "/home/joacosk/Documents/maestria/Q2/script_project/.venv/lib/python3.12/site-packages/google/cloud/bigquery/_pandas_helpers.py:484: FutureWarning: Loading pandas DataFrame into BigQuery will require pandas-gbq package version 0.26.1 or greater in the future. Tried to import pandas-gbq and got: No module named 'pandas_gbq'\n",
      "  warnings.warn(\n",
      "INFO:__main__:✅ DataFrame cargado exitosamente a BigQuery en la tabla 'c02' en el dataset 'dmeyf'.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-21T16:56:09.389246Z",
     "start_time": "2025-11-21T16:56:09.374009Z"
    }
   },
   "source": [
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def create_churn_targets_bq(\n",
    "    project_id: str,\n",
    "    dataset_id: str,\n",
    "    source_table: str,\n",
    "    target_table: str = \"targets\",\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Crea la tabla de targets (clase_ternaria) en BigQuery utilizando la lógica\n",
    "    de gap temporal (BAJA+1, BAJA+2) del código R/data.table.\n",
    "\n",
    "    Args:\n",
    "        project_id: ID del proyecto de Google Cloud.\n",
    "        dataset_id: ID del dataset.\n",
    "        source_table: Nombre de la tabla de datos crudos o features.\n",
    "        target_table: Nombre de la tabla donde se guardarán los targets.\n",
    "    \"\"\"\n",
    "\n",
    "    logger.info(f\"Iniciando creación de tabla de targets '{target_table}'...\")\n",
    "\n",
    "    try:\n",
    "        client = bigquery.Client(project=project_id)\n",
    "        source_ref = f\"`{project_id}.{dataset_id}.{source_table}`\"\n",
    "        target_ref = f\"`{project_id}.{dataset_id}.{target_table}`\"\n",
    "\n",
    "        # SQL para replicar la lógica de Target Engineering (R/data.table)\n",
    "        query = f\"\"\"\n",
    "        CREATE OR REPLACE TABLE {target_ref}\n",
    "        PARTITION BY RANGE_BUCKET(foto_mes, GENERATE_ARRAY(201901, 202208, 1))\n",
    "        CLUSTER BY foto_mes, numero_de_cliente\n",
    "        AS\n",
    "        WITH PreCalculations AS (\n",
    "            SELECT\n",
    "                foto_mes,\n",
    "                numero_de_cliente,\n",
    "                -- 1. Calcula el periodo serializado (ej. 202401 -> 24289)\n",
    "                CAST(FLOOR(t1.foto_mes / 100) AS INT64) * 12 + MOD(t1.foto_mes, 100) AS periodo0,\n",
    "                -- 2. Calcula los leads (periodo1 y periodo2)\n",
    "                LEAD(CAST(FLOOR(t1.foto_mes / 100) AS INT64) * 12 + MOD(t1.foto_mes, 100), 1)\n",
    "                    OVER (PARTITION BY t1.numero_de_cliente ORDER BY t1.foto_mes) AS periodo1,\n",
    "                LEAD(CAST(FLOOR(t1.foto_mes / 100) AS INT64) * 12 + MOD(t1.foto_mes, 100), 2)\n",
    "                    OVER (PARTITION BY t1.numero_de_cliente ORDER BY t1.foto_mes) AS periodo2\n",
    "            FROM {source_ref} AS t1\n",
    "        ),\n",
    "        MaxPeriods AS (\n",
    "            SELECT\n",
    "                MAX(periodo0) AS periodo_ultimo,\n",
    "                MAX(periodo0) - 1 AS periodo_anteultimo\n",
    "            FROM PreCalculations\n",
    "        )\n",
    "        SELECT\n",
    "            t1.foto_mes,\n",
    "            t1.numero_de_cliente,\n",
    "            t1.periodo0,\n",
    "            t1.periodo1,\n",
    "            t1.periodo2,\n",
    "            -- 3. Aplica la lógica de la clase ternaria (siguiendo precedencia)\n",
    "            CASE\n",
    "                -- BAJA+2: Antes del penúltimo mes, hay continuidad en M+1, pero falta M+2 (gap > 2)\n",
    "                WHEN t1.periodo0 < mp.periodo_anteultimo AND\n",
    "                     (t1.periodo0 + 1 = t1.periodo1) AND\n",
    "                     (t1.periodo2 IS NULL OR t1.periodo0 + 2 < t1.periodo2)\n",
    "                THEN 'BAJA+2'\n",
    "\n",
    "                -- BAJA+1: Antes del último mes, falta M+1 (periodo1 es NULL o hay un gap)\n",
    "                WHEN t1.periodo0 < mp.periodo_ultimo AND\n",
    "                     (t1.periodo1 IS NULL OR t1.periodo0 + 1 < t1.periodo1)\n",
    "                THEN 'BAJA+1'\n",
    "\n",
    "                -- CONTINUA: Por defecto para registros antes del penúltimo mes que no son Baja\n",
    "                WHEN t1.periodo0 < mp.periodo_anteultimo\n",
    "                THEN 'CONTINUA'\n",
    "\n",
    "                ELSE NULL -- Registros del último/penúltimo mes sin clasificación de Baja\n",
    "            END AS clase_ternaria\n",
    "        FROM PreCalculations AS t1\n",
    "        CROSS JOIN MaxPeriods AS mp;\n",
    "        \"\"\"\n",
    "\n",
    "        job = client.query(query)\n",
    "        job.result()\n",
    "        logger.info(f\"✅ Tabla de targets '{target_table}' creada exitosamente.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Error al crear la tabla de targets en BigQuery: {e}\")\n",
    "        raise\n",
    "\n",
    "# Nota: Esta función solo crea la tabla de targets, las features se procesarán en la siguiente función."
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T16:33:58.175801Z",
     "start_time": "2025-11-21T16:33:49.109396Z"
    }
   },
   "cell_type": "code",
   "source": "create_churn_targets_bq(config.BQ_PROJECT, config.BQ_DATASET, \"c02\", \"targets\")",
   "id": "84bd4061b2ae5812",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Iniciando creación de tabla de targets 'targets'...\n",
      "INFO:__main__:✅ Tabla de targets 'targets' creada exitosamente.\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from src.loader import tabla_productos_por_cliente\n",
    "tabla_productos_por_cliente(config.BQ_PROJECT, config.BQ_DATASET, config.BQ_TABLE,\n",
    "                                    config.BQ_TABLE_TARGETS)\n",
    "# crea tabla c02_products"
   ],
   "id": "dc15917f0055dca2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T16:56:13.156799Z",
     "start_time": "2025-11-21T16:56:13.147413Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_intra_month_features_bq(\n",
    "    project_id: str,\n",
    "    dataset_id: str,\n",
    "    source_table: str,\n",
    "    output_table: str\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Crea la tabla con Feature Engineering intra-mes en BigQuery.\n",
    "\n",
    "    Args:\n",
    "        project_id: ID del proyecto de Google Cloud.\n",
    "        dataset_id: ID del dataset.\n",
    "        source_table: Nombre de la tabla de entrada (cruda o con targets).\n",
    "        output_table: Nombre de la tabla de salida.\n",
    "    \"\"\"\n",
    "\n",
    "    logger.info(f\"Iniciando Feature Engineering intra-mes para '{output_table}'...\")\n",
    "\n",
    "    try:\n",
    "        client = bigquery.Client(project=project_id)\n",
    "        source_ref = f\"`{project_id}.{dataset_id}.{source_table}`\"\n",
    "        output_ref = f\"`{project_id}.{dataset_id}.{output_table}`\"\n",
    "\n",
    "        query = f\"\"\"\n",
    "        CREATE OR REPLACE TABLE {output_ref}\n",
    "        PARTITION BY RANGE_BUCKET(foto_mes, GENERATE_ARRAY(201901, 202208, 1))\n",
    "        CLUSTER BY foto_mes, numero_de_cliente\n",
    "        AS\n",
    "        SELECT\n",
    "            t1.* EXCEPT(clase_ternaria),\n",
    "            t1.clase_ternaria, -- Aseguramos que la columna clase_ternaria esté al final\n",
    "\n",
    "            -- kmes (Mes del año)\n",
    "            MOD(t1.foto_mes, 100) AS kmes,\n",
    "\n",
    "            -- ctrx_quarter_normalizado (Normalización de ctrx_quarter por antigüedad)\n",
    "            CASE\n",
    "                WHEN t1.cliente_antiguedad = 1 THEN t1.ctrx_quarter * 5.0\n",
    "                WHEN t1.cliente_antiguedad = 2 THEN t1.ctrx_quarter * 2.0\n",
    "                WHEN t1.cliente_antiguedad = 3 THEN t1.ctrx_quarter * 1.2\n",
    "                ELSE t1.ctrx_quarter -- Valor por defecto o ctrx_quarter original\n",
    "            END AS ctrx_quarter_normalizado,\n",
    "\n",
    "            -- mpayroll_sobre_edad\n",
    "            CASE\n",
    "                WHEN t1.cliente_edad IS NULL OR t1.cliente_edad = 0 THEN NULL\n",
    "                ELSE t1.mpayroll / t1.cliente_edad\n",
    "            END AS mpayroll_sobre_edad\n",
    "\n",
    "        FROM {source_ref} AS t1;\n",
    "        \"\"\"\n",
    "\n",
    "        job = client.query(query)\n",
    "        job.result()\n",
    "        logger.info(f\"✅ Feature Engineering intra-mes completado. Tabla guardada en '{output_table}'.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Error al ejecutar el Feature Engineering intra-mes en BigQuery: {e}\")\n",
    "        raise"
   ],
   "id": "bb010944cf45c92a",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T16:56:23.495659Z",
     "start_time": "2025-11-21T16:56:14.435729Z"
    }
   },
   "cell_type": "code",
   "source": [
    "create_intra_month_features_bq(config.BQ_PROJECT, config.BQ_DATASET, 'c02_products',\n",
    "                                    config.BQ_TABLE_FEATURES)"
   ],
   "id": "446aaa296493f92e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Iniciando Feature Engineering intra-mes para 'c02_features'...\n",
      "INFO:__main__:✅ Feature Engineering intra-mes completado. Tabla guardada en 'c02_features'.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T17:40:05.917508Z",
     "start_time": "2025-11-21T17:40:05.897632Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ESTÁ EN FEATURES.PY\n",
    "\n",
    "def create_historical_features_bq(\n",
    "    project_id: str,\n",
    "    dataset_id: str,\n",
    "    source_table: str,\n",
    "    output_table: str,\n",
    "    cols_to_engineer: list, # Lista de columnas para las que calcular historial\n",
    "    window_size: int = 6,\n",
    ") -> None:\n",
    "\n",
    "    logger.info(f\"Iniciando Feature Engineering histórico ({window_size} meses) con CTEs para evitar anidación...\")\n",
    "\n",
    "    try:\n",
    "        client = bigquery.Client(project=project_id)\n",
    "        source_ref = f\"`{project_id}.{dataset_id}.{source_table}`\"\n",
    "        output_ref = f\"`{project_id}.{dataset_id}.{output_table}`\"\n",
    "\n",
    "        # --- CONSTRUCCIÓN DINÁMICA DE EXPRESIONES (Sin la cláusula 'OVER') ---\n",
    "        lag_exprs = []\n",
    "        hist_exprs = []\n",
    "\n",
    "        # 1. Definición de la especificación de la ventana (para re-uso)\n",
    "        window_spec_name = \"w\"\n",
    "        window_spec_sql = f\"\"\"\n",
    "            WINDOW {window_spec_name} AS (\n",
    "                PARTITION BY numero_de_cliente\n",
    "                ORDER BY foto_mes\n",
    "                ROWS BETWEEN {window_size - 1} PRECEDING AND CURRENT ROW\n",
    "            )\n",
    "        \"\"\"\n",
    "\n",
    "        # 2. Loop para generar todas las expresiones (usando alias 't2')\n",
    "        for col in cols_to_engineer:\n",
    "\n",
    "            # --- Variables limpias del CTE BaseFeatures (t2) ---\n",
    "            col_base = f\"t2_{col}_clean\"\n",
    "            col_rn = \"t2_row_index\"\n",
    "\n",
    "            # --- 1. Lags ---\n",
    "            col_lag1 = f\"LAG({col_base}, 1) OVER (PARTITION BY t2.numero_de_cliente ORDER BY t2.foto_mes)\"\n",
    "            col_lag2 = f\"LAG({col_base}, 2) OVER (PARTITION BY t2.numero_de_cliente ORDER BY t2.foto_mes)\"\n",
    "\n",
    "            lag_exprs.append(f\"{col_lag1} AS {col}_lag1\")\n",
    "            lag_exprs.append(f\"{col_lag2} AS {col}_lag2\")\n",
    "\n",
    "            # --- 2. Deltas (Resta) ---\n",
    "            lag_exprs.append(f\"({col_base} - {col_lag1}) AS {col}_delta1\")\n",
    "            lag_exprs.append(f\"({col_base} - {col_lag2}) AS {col}_delta2\")\n",
    "\n",
    "            # --- 3. Tendencia (COVAR_POP / VAR_POP) ---\n",
    "            # Aplicamos la ventana NOMBRADA ({window_spec_name}) a cada función de agregación\n",
    "            hist_exprs.append(f\"\"\"\n",
    "                (\n",
    "                    COVAR_POP({col_base}, {col_rn}) OVER {window_spec_name}\n",
    "                    /\n",
    "                    NULLIF(VAR_POP({col_rn}) OVER {window_spec_name}, 0)\n",
    "                ) AS {col}_tend{window_size}\n",
    "            \"\"\")\n",
    "\n",
    "            # --- 4. Promedio, Min, Max (AVG, MIN, MAX) ---\n",
    "            col_avg = f\"AVG({col_base}) OVER {window_spec_name}\"\n",
    "            col_min = f\"MIN({col_base}) OVER {window_spec_name}\"\n",
    "            col_max = f\"MAX({col_base}) OVER {window_spec_name}\"\n",
    "\n",
    "            hist_exprs.append(f\"{col_avg} AS {col}_avg{window_size}\")\n",
    "            # Corregir los nombres de alias de min/max (antes eran _avg{window_size} duplicados)\n",
    "            hist_exprs.append(f\"{col_min} AS {col}_min{window_size}\")\n",
    "            hist_exprs.append(f\"{col_max} AS {col}_max{window_size}\")\n",
    "\n",
    "            # --- 5. Ratios (División) ---\n",
    "            hist_exprs.append(f\"({col_base} / NULLIF({col_avg}, 0)) AS {col}_ratioavg{window_size}\")\n",
    "            hist_exprs.append(f\"({col_base} / NULLIF({col_max}, 0)) AS {col}_ratiomax{window_size}\")\n",
    "\n",
    "        all_new_features = lag_exprs + hist_exprs\n",
    "\n",
    "        # --- QUERY FINAL CON CTEs ---\n",
    "        # BaseFeatures: Limpieza de tipos y cálculo del índice (ROW_NUMBER)\n",
    "        # HistoricalFeatures: Cálculos de ventana (LAG, AVG, COVAR)\n",
    "\n",
    "        cols_for_base_cte = [f\"CAST(t1.{col} AS FLOAT64) AS t2_{col}_clean\" for col in cols_to_engineer]\n",
    "\n",
    "        query = f\"\"\"\n",
    "        CREATE OR REPLACE TABLE {output_ref}\n",
    "        PARTITION BY RANGE_BUCKET(foto_mes, GENERATE_ARRAY(201801, 203001, 1))\n",
    "        CLUSTER BY foto_mes, numero_de_cliente\n",
    "        AS\n",
    "        WITH BaseFeatures AS (\n",
    "            SELECT\n",
    "                -- 1. Seleccionamos TODAS las columnas originales (solo una vez)\n",
    "                t1.*,\n",
    "                -- 2. Calculamos el índice para la regresión (X)\n",
    "                ROW_NUMBER() OVER(PARTITION BY t1.numero_de_cliente ORDER BY t1.foto_mes) AS t2_row_index,\n",
    "                -- 3. Creamos las versiones limpias/casteadas de las features (Y)\n",
    "                {', '.join(cols_for_base_cte)}\n",
    "            FROM {source_ref} AS t1\n",
    "        ),\n",
    "        HistoricalFeatures AS (\n",
    "            SELECT\n",
    "                -- 1. Seleccionamos todas las columnas base y eliminamos las auxiliares\n",
    "                t2.* EXCEPT({', '.join([f\"t2_{col}_clean\" for col in cols_to_engineer])}, t2_row_index),\n",
    "\n",
    "                -- 2. Agregamos las features históricas calculadas\n",
    "                {', '.join(all_new_features)}\n",
    "            FROM BaseFeatures AS t2\n",
    "            {window_spec_sql}\n",
    "        )\n",
    "        SELECT * FROM HistoricalFeatures;\n",
    "        \"\"\"\n",
    "        # DEBUG: Imprimir la query completa para revisión antes de ejecutar\n",
    "        # print(query)\n",
    "\n",
    "        job = client.query(query)\n",
    "        job.result()\n",
    "        logger.info(f\"✅ Feature Engineering histórico completado. Tabla guardada en '{output_table}'.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Error al ejecutar el Feature Engineering histórico en BigQuery: {e}\")\n",
    "        raise"
   ],
   "id": "4b9084afa47a3cd5",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T17:40:07.056099Z",
     "start_time": "2025-11-21T17:40:07.039919Z"
    }
   },
   "cell_type": "code",
   "source": [
    "LAG_VARS = ['Master_cconsumos', 'Master_fultimo_cierre', 'Master_mconsumospesos', 'Master_mfinanciacion_limite',\n",
    " 'Master_mlimitecompra', 'Master_mpagominimo', 'Master_mpagospesos', 'Master_msaldopesos', 'Master_msaldototal',\n",
    " 'Visa_Fvencimiento', 'Visa_cconsumos', 'Visa_mconsumosdolares', 'Visa_mconsumospesos',\n",
    " 'Visa_mconsumototal', 'Visa_mfinanciacion_limite', 'Visa_mlimitecompra', 'Visa_mpagado',\n",
    " 'Visa_mpagominimo', 'Visa_mpagospesos', 'Visa_msaldopesos', 'Visa_msaldototal', 'Visa_status', 'ccaja_ahorro',\n",
    " 'ccaja_seguridad', 'ccajas_consultas', 'ccajas_otras', 'ccajas_transacciones', 'ccallcenter_transacciones',\n",
    " 'ccomisiones_mantenimiento', 'ccomisiones_otras', 'ccuenta_debitos_automaticos', 'cdescubierto_preacordado', 'cextraccion_autoservicio', 'chomebanking_transacciones', 'cmobile_app_trx', 'cpagomiscuentas',\n",
    " 'cpayroll_trx', 'cprestamos_personales', 'cproductos', 'ctarjeta_debito', 'ctarjeta_visa', 'ctarjeta_visa_debitos_automaticos',\n",
    " 'ctarjeta_visa_transacciones', 'ctransferencias_emitidas', 'ctransferencias_recibidas',\n",
    " 'ctrx_quarter', 'mactivos_margen', 'mautoservicio', 'mcaja_ahorro', 'mcaja_ahorro_dolares',\n",
    " 'mcomisiones', 'mcomisiones_mantenimiento', 'mcomisiones_otras', 'mcuenta_corriente', 'mcuenta_debitos_automaticos', 'mcuentas_saldo', 'mextraccion_autoservicio', 'mpagomiscuentas', 'mpasivos_margen',\n",
    " 'mpayroll', 'mplazo_fijo_dolares', 'mprestamos_personales', 'mrentabilidad', 'mrentabilidad_annual',\n",
    " 'mtarjeta_master_consumo', 'mtarjeta_visa_consumo', 'mtransferencias_emitidas', 'mtransferencias_recibidas', 'mttarjeta_visa_debitos_automaticos', 'tcallcenter', 'thomebanking', 'tmobile_app']\n",
    "\n",
    "# Listo aparte las features creadas\n",
    "features_nuevas = [\"q_producto_master\", \"q_producto_visa\",\"q_producto_general\",\"ctrx_quarter_normalizado\",\"mpayroll_sobre_edad\" ]\n",
    "\n",
    "# Agrego las features creadas\n",
    "LAG_VARS += features_nuevas\n",
    "\n",
    "# Dejo valores únicos\n",
    "LAG_VARS = list(set(LAG_VARS))\n"
   ],
   "id": "566959da6ac9096b",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T17:43:00.296264Z",
     "start_time": "2025-11-21T17:40:08.007613Z"
    }
   },
   "cell_type": "code",
   "source": [
    "create_historical_features_bq(\n",
    "        config.BQ_PROJECT,\n",
    "        config.BQ_DATASET,\n",
    "        config.BQ_TABLE_FEATURES,\n",
    "        'c02_features_historical',\n",
    "        cols_to_engineer=LAG_VARS,\n",
    "        window_size=6\n",
    "    )"
   ],
   "id": "5c64f387af6bd3ef",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Iniciando Feature Engineering histórico (6 meses) con CTEs para evitar anidación...\n",
      "INFO:__main__:✅ Feature Engineering histórico completado. Tabla guardada en 'c02_features_historical'.\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Guardado de parquet",
   "id": "d4cd8330cb78aa4f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T19:37:36.690752Z",
     "start_time": "2025-11-21T19:37:27.268978Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from google.cloud import bigquery\n",
    "from google.cloud.bigquery import job\n",
    "import logging\n",
    "from typing import List\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def export_bq_to_gcs_native(\n",
    "    project_id: str,\n",
    "    dataset_id: str,\n",
    "    source_table: str,\n",
    "    gcs_path_proc: str, # Debe contener el comodín '*' para fragmentación\n",
    "    select_cols: List[str] = ['*']\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Exporta una tabla de BigQuery directamente a archivos Parquet fragmentados en GCS\n",
    "    utilizando la API nativa de BigQuery (sin DuckDB).\n",
    "\n",
    "    Args:\n",
    "        project_id: ID del proyecto de Google Cloud.\n",
    "        dataset_id: ID del dataset de BQ.\n",
    "        source_table: Nombre de la tabla de BQ a exportar.\n",
    "        gcs_path_proc: Ruta completa de GCS de destino (ej. 'gs://bucket/path-*.parquet').\n",
    "        select_cols: Lista de columnas a exportar.\n",
    "    \"\"\"\n",
    "\n",
    "    logger.info(f\"Iniciando exportación nativa: BQ ({source_table}) -> GCS Parquet ({gcs_path_proc})\")\n",
    "\n",
    "    try:\n",
    "        client = bigquery.Client(project=project_id)\n",
    "\n",
    "        # 1. Crear una vista temporal (si es necesario) para seleccionar solo ciertas columnas\n",
    "        # Si select_cols != ['*'], necesitamos una Query para filtrar columnas/datos\n",
    "        if select_cols != ['*']:\n",
    "            cols_str = ', '.join(select_cols)\n",
    "            source_query = f\"SELECT {cols_str} FROM `{project_id}.{dataset_id}.{source_table}`\"\n",
    "\n",
    "            # Ejecutar la consulta y guardar el resultado en una tabla temporal (o usar la tabla de destino como origen)\n",
    "            # Para exportación, es más fácil si la tabla de origen ya está lista.\n",
    "            # Si solo se filtra, se usa un ExtractJob\n",
    "            logger.warning(\"La exportación nativa no soporta la selección de columnas 'SELECT' directamente sobre la tabla de origen.\")\n",
    "            logger.warning(\"Se exportará la tabla completa. Si necesita filtrar, use un CTE o cree una tabla/vista intermedia.\")\n",
    "\n",
    "        table_ref = client.dataset(dataset_id).table(source_table)\n",
    "\n",
    "        # 2. Configurar el Job de Exportación\n",
    "        job_config = job.ExtractJobConfig()\n",
    "        job_config.destination_format = bigquery.DestinationFormat.PARQUET\n",
    "        job_config.compression = 'SNAPPY' # La compresión estándar para Parquet\n",
    "\n",
    "        # 3. Ejecutar la exportación\n",
    "        extract_job = client.extract_table(\n",
    "            table_ref,\n",
    "            gcs_path_proc, # Ya contiene el comodín '*'\n",
    "            job_config=job_config\n",
    "        )\n",
    "        extract_job.result()  # Esperar a que el job termine\n",
    "\n",
    "        logger.info(f\"✅ Exportación nativa completada. Archivos guardados en: {gcs_path_proc.replace('*', '...')}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Error durante la exportación nativa de BQ: {e}\")\n",
    "        # El error 400 'Table too large' ya fue resuelto con el comodín,\n",
    "        # pero es bueno saber que este método lo maneja.\n",
    "        raise\n",
    "\n",
    "# --- Ejemplo de Uso ---\n",
    "if __name__ == '__main__':\n",
    "    # Usar la misma ruta fragmentada que causó el error en DuckDB\n",
    "    GCS_DESTINO = \"gs://joaquinrk_data_bukito3/datasets/competencia_02_features/competencia_02_features-*.parquet\"\n",
    "\n",
    "    try:\n",
    "        # Reemplaza con tus variables reales\n",
    "        export_bq_to_gcs_native(\n",
    "            project_id=\"dmecoyfin-250928192534125\",\n",
    "            dataset_id=\"dmeyf\",\n",
    "            source_table=\"c02_features_historical\",\n",
    "            gcs_path_proc=GCS_DESTINO,\n",
    "            select_cols=['foto_mes', 'numero_de_cliente', 'clase_ternaria', 'feature_1_avg6', 'feature_2_tend6']\n",
    "        )\n",
    "    except Exception:\n",
    "        pass"
   ],
   "id": "f255d82e0ab8b4",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Iniciando exportación nativa: BQ (c02_features_historical) -> GCS Parquet (gs://joaquinrk_data_bukito3/datasets/competencia_02_features/competencia_02_features-*.parquet)\n",
      "WARNING:__main__:La exportación nativa no soporta la selección de columnas 'SELECT' directamente sobre la tabla de origen.\n",
      "WARNING:__main__:Se exportará la tabla completa. Si necesita filtrar, use un CTE o cree una tabla/vista intermedia.\n",
      "INFO:__main__:✅ Exportación nativa completada. Archivos guardados en: gs://joaquinrk_data_bukito3/datasets/competencia_02_features/competencia_02_features-....parquet\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T19:46:09.048852Z",
     "start_time": "2025-11-21T19:46:07.104415Z"
    }
   },
   "cell_type": "code",
   "source": [
    "path = \"gs://joaquinrk_data_bukito3/datasets/competencia_02_features/competencia_02_features-*.parquet\"\n",
    "# O un directorio entero\n",
    "# path = \"./carpeta/*.parquet\"\n",
    "gcs_bearer_token = _aut_google()\n",
    "\n",
    "con = duckdb.connect(database=':memory:', read_only=False)\n",
    "con.sql(\"INSTALL httpfs;\")\n",
    "con.sql(\"LOAD httpfs;\")\n",
    "logger.info(\"DuckDB conectado y extensión 'httpfs' cargada.\")\n",
    "\n",
    "\n",
    "con.sql(f\"SET s3_region='';\")\n",
    "con.sql(f\"SET s3_access_key_id='';\")\n",
    "con.sql(f\"SET s3_secret_access_key='';\")\n",
    "con.sql(f\"SET s3_endpoint='';\")\n",
    "con.sql(f\"SET s3_session_token='';\")\n",
    "con.sql(f\"SET s3_url_style='path';\")\n",
    "con.sql(f\"SET gcs_bearer_token='{gcs_bearer_token}';\") # <--- Inyectamos el token aquí\n",
    "logger.info(\"DuckDB GCS configurado usando bearer token (ADC).\")\n",
    "\n",
    "logger.info(\"DuckDB conectado, extensión 'httpfs' cargada y secreto GCS creado.\")\n",
    "\n",
    "# Ejecutar una consulta SQL directamente sobre los archivos\n",
    "# DuckDB interpreta el patrón como un conjunto de datos único\n",
    "query = f\"SELECT * FROM read_parquet('{path}') where foto_mes = 202101 LIMIT 10\"\n",
    "\n",
    "# Ejecutar la consulta y obtener los resultados en un DataFrame de Pandas\n",
    "# DuckDB es muy eficiente leyendo los metadatos y filtrando solo lo necesario\n",
    "df = conn.execute(query).fetchdf()\n",
    "\n",
    "print(df.head())\n",
    "print(f\"Total de filas leídas: {len(df)}\")\n"
   ],
   "id": "31c83666034aa712",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Token de Google Cloud obtenido exitosamente.\n",
      "INFO:__main__:DuckDB conectado y extensión 'httpfs' cargada.\n"
     ]
    },
    {
     "ename": "CatalogException",
     "evalue": "Catalog Error: unrecognized configuration parameter \"gcs_bearer_token\"\n\nDid you mean: \"s3_region\"",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mCatalogException\u001B[39m                          Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[68]\u001B[39m\u001B[32m, line 18\u001B[39m\n\u001B[32m     16\u001B[39m con.sql(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mSET s3_session_token=\u001B[39m\u001B[33m'\u001B[39m\u001B[33m'\u001B[39m\u001B[33m;\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     17\u001B[39m con.sql(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mSET s3_url_style=\u001B[39m\u001B[33m'\u001B[39m\u001B[33mpath\u001B[39m\u001B[33m'\u001B[39m\u001B[33m;\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m18\u001B[39m \u001B[43mcon\u001B[49m\u001B[43m.\u001B[49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43mf\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mSET gcs_bearer_token=\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mgcs_bearer_token\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m'\u001B[39;49m\u001B[33;43m;\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;66;03m# <--- Inyectamos el token aquí\u001B[39;00m\n\u001B[32m     19\u001B[39m logger.info(\u001B[33m\"\u001B[39m\u001B[33mDuckDB GCS configurado usando bearer token (ADC).\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     21\u001B[39m logger.info(\u001B[33m\"\u001B[39m\u001B[33mDuckDB conectado, extensión \u001B[39m\u001B[33m'\u001B[39m\u001B[33mhttpfs\u001B[39m\u001B[33m'\u001B[39m\u001B[33m cargada y secreto GCS creado.\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[31mCatalogException\u001B[39m: Catalog Error: unrecognized configuration parameter \"gcs_bearer_token\"\n\nDid you mean: \"s3_region\""
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "13481a600a70057e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
