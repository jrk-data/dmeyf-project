# ğŸ“˜ Clase02.md

## **ModularizaciÃ³n: separar responsabilidades (pandas + SQL, con logging)**

---

## Git - Flujo de trabajo bÃ¡sico

Antes de comenzar con la clase, es importante mantener nuestro repositorio actualizado y sincronizado. Ejecuta los siguientes comandos en la consola:

```bash
git status
git add .
git commit -m "actualizacion"
git pull origin
git status
git add .
git push
```

**ExplicaciÃ³n del flujo:**

1. `git status` - Verifica el estado actual del repositorio, mostrando archivos modificados, agregados o eliminados
2. `git add .` - Agrega todos los cambios locales al Ã¡rea de staging (preparaciÃ³n para commit)
3. `git commit -m "actualizacion"` - Crea un commit con los cambios locales y un mensaje descriptivo
4. `git pull origin` - Descarga y fusiona los cambios mÃ¡s recientes del repositorio remoto
5. `git status` - Verifica nuevamente el estado despuÃ©s del pull para detectar posibles conflictos
6. `git add .` - Agrega cualquier archivo que pueda haber sido modificado durante la fusiÃ³n
7. `git push` - Sube los cambios locales al repositorio remoto

Este flujo asegura que tu trabajo estÃ© sincronizado con el repositorio principal y evita conflictos de versiones.

**Al finalizar la clase, ejecuta nuevamente:**

```bash
git add .
git commit -m "Clase02: ModularizaciÃ³n completada - separaciÃ³n de responsabilidades con logging"
git push
```

---

## ğŸ¯ Objetivo

Entender que **cada archivo tiene un rol** y orquestar:
**cargar â†’ transformar (SQL) â†’ loguear â†’ guardar**, ahora con **logging profesional**.

---

## ğŸ“‘ Ãndice de la clase

1. **Estructura de carpetas** - Organizaremos el proyecto con una arquitectura clara que separe datos, cÃ³digo fuente, logs y resultados
2. **ConfiguraciÃ³n de logs con `logging`** - Implementaremos un sistema de logging profesional para rastrear la ejecuciÃ³n y detectar errores
3. **FunciÃ³n en `main.py` para carga de dataset** - Crearemos una funciÃ³n bÃ¡sica de carga con documentaciÃ³n clara de parÃ¡metros y valores de retorno
4. **Crear `src/loader.py` y trasladar la funciÃ³n `cargar_datos(path)`** - Modularizaremos el cÃ³digo moviendo la lÃ³gica de carga a un mÃ³dulo especializado
5. **Importaciones y `__init__.py`** - Configuraremos el sistema de paquetes Python para permitir importaciones limpias entre mÃ³dulos
6. **`src/features.py`: SQL directo (DuckDB) para generar `Lag`** - Implementaremos feature engineering usando SQL para crear variables de rezago temporal
7. **`main.py` orquestando todo con logs** - Integraremos todos los mÃ³dulos en un flujo principal con logging completo

---

## 1) Estructura de carpetas

La organizaciÃ³n del proyecto es fundamental para mantener un cÃ³digo limpio y escalable. Esta estructura separa claramente las responsabilidades:

```
proyecto_ml/
â”œâ”€â”€ main.py              # Archivo principal que orquesta todo el flujo
â”œâ”€â”€ requirements.txt     # Dependencias del proyecto
â”œâ”€â”€ data/               # Carpeta para datasets de entrada y salida
â”‚   â””â”€â”€ competencia_01.csv
â”œâ”€â”€ logs/               # Archivos de log para debugging y monitoreo
â”œâ”€â”€ output/             # Resultados finales y archivos procesados
â””â”€â”€ src/                # CÃ³digo fuente modularizado
    â”œâ”€â”€ __init__.py     # Convierte src/ en un paquete Python
    â”œâ”€â”€ loader.py       # MÃ³dulo especializado en carga de datos
    â””â”€â”€ features.py     # MÃ³dulo para feature engineering
```

**Beneficios de esta estructura:**

- **SeparaciÃ³n de responsabilidades**: Cada carpeta tiene un propÃ³sito especÃ­fico
- **Escalabilidad**: FÃ¡cil agregar nuevos mÃ³dulos en `src/`
- **Mantenibilidad**: CÃ³digo organizado y fÃ¡cil de encontrar
- **Profesionalismo**: Estructura estÃ¡ndar en proyectos

## 2) ConfiguraciÃ³n de logs con `logging`

El sistema de logging es esencial para el debugging y monitoreo de aplicaciones en producciÃ³n. Python ofrece el mÃ³dulo `logging` que permite registrar eventos durante la ejecuciÃ³n del programa.

**Â¿Por quÃ© usar logging en lugar de print()?**

- **Niveles de severidad**: DEBUG, INFO, WARNING, ERROR, CRITICAL
- **Formato personalizable**: Timestamps, nombres de mÃ³dulos, nÃºmeros de lÃ­nea
- **MÃºltiples destinos**: Archivo, consola, servicios remotos
- **Control granular**: Activar/desactivar logs por mÃ³dulo o nivel

**DocumentaciÃ³n oficial**: https://docs.python.org/3/howto/logging.html

```python
import pandas as pd
import os
import datetime
import logging

from src.loader import cargar_datos
from src.features import feature_engineering_lag

## config basico logging
os.makedirs("logs", exist_ok=True)

fecha = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
monbre_log = f"log_{fecha}.log"
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(levelname)s - %(name)s %(lineno)d - %(message)s',
    handlers=[
        logging.FileHandler(f"logs/{monbre_log}", mode="w", encoding="utf-8"),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)
```

---

## 3) FunciÃ³n en `main.py`: carga de dataset

En esta etapa creamos una funciÃ³n bÃ¡sica para cargar datos. Es importante documentar claramente los parÃ¡metros de entrada y el valor de retorno usando **type hints** y **docstrings**.

**Conceptos clave:**

- **Type hints**: Especifican el tipo de datos esperado (`str`, `pd.DataFrame`)
- **Docstring**: DocumentaciÃ³n integrada que explica quÃ© hace la funciÃ³n
- **Valor de retorno**: Siempre especificar quÃ© devuelve la funciÃ³n

```python
# main.py (paso didÃ¡ctico inicial)

import pandas as pd

def cargar_datos(path: str) -> pd.DataFrame:
    '''
    Carga un CSV desde 'path' y retorna un pandas.DataFrame.
    '''
    df = pd.read_csv(path)
    return df

if __name__ == "__main__":
    df = cargar_datos("data/competencia_01.csv")
    print(df.head())
```

---

## 4) Crear `src/loader.py` y trasladar la funciÃ³n

La **modularizaciÃ³n** es clave en el desarrollo de software. Movemos la funciÃ³n de carga a un mÃ³dulo especializado para:

**Ventajas de la modularizaciÃ³n:**

- **ReutilizaciÃ³n**: La funciÃ³n puede usarse en otros scripts
- **Mantenimiento**: Cambios en la lÃ³gica de carga solo afectan un archivo
- **Testing**: MÃ¡s fÃ¡cil crear pruebas unitarias para funciones especÃ­ficas
- **ColaboraciÃ³n**: Diferentes desarrolladores pueden trabajar en mÃ³dulos separados

**Mejoras en esta versiÃ³n:**

- **Manejo de errores**: Try-catch para capturar problemas de carga
- **Logging integrado**: Registra el proceso de carga y posibles errores
- **Type hints mejorados**: `pd.DataFrame | None` indica que puede retornar None en caso de error

```python
# src/loader.py
import pandas as pd
import logging

logger = logging.getLogger("__name__")

## Funcion para cargar datos
def cargar_datos(path: str) -> pd.DataFrame | None:

    '''
    Carga un CSV desde 'path' y retorna un pandas.DataFrame.
    '''

    logger.info(f"Cargando dataset desde {path}")
    try:
        df = pd.read_csv(path)
        logger.info(f"Dataset cargado con {df.shape[0]} filas y {df.shape[1]} columnas")
        return df
    except Exception as e:
        logger.error(f"Error al cargar el dataset: {e}")
        raise
```

---

## 5) Importaciones y `__init__.py`

El archivo `__init__.py` convierte una carpeta en un **paquete Python**, permitiendo importaciones limpias y organizadas.

**Â¿Por quÃ© es importante `__init__.py`?**

- **Reconocimiento de paquete**: Python identifica la carpeta como un mÃ³dulo importable
- **Importaciones limpias**: Permite usar `from src.loader import cargar_datos`
- **Namespace control**: Define quÃ© funciones estÃ¡n disponibles al importar el paquete
- **InicializaciÃ³n**: Puede ejecutar cÃ³digo cuando se importa el paquete por primera vez

**Buenas prÃ¡cticas:**

- Mantener `__init__.py` vacÃ­o o con cÃ³digo mÃ­nimo de inicializaciÃ³n
- Usar importaciones absolutas para mayor claridad
- Documentar las dependencias entre mÃ³dulos

En `main.py`:

```python
from src.loader import cargar_datos
from src.features import generar_features
```

Archivo vacÃ­o `src/__init__.py`:

```python
# src/__init__.py
# (intencionalmente vacÃ­o)
```

---

## 6) `src/features.py`: SQL directo (DuckDB) para generar `Lag`

**Feature Engineering** es el proceso de crear nuevas variables a partir de datos existentes. En este caso, usamos **DuckDB** para generar variables de **lag** (rezago temporal) usando SQL.

**Â¿QuÃ© son las variables Lag?**

- **Lag 1**: Valor de la variable en el perÃ­odo anterior
- **Lag 2**: Valor de la variable hace 2 perÃ­odos
- **Utilidad**: Capturan patrones temporales y tendencias histÃ³ricas

**Â¿Por quÃ© usar DuckDB?**

- **Performance**: Mucho mÃ¡s rÃ¡pido que pandas para operaciones complejas
- **SQL familiar**: Sintaxis conocida para transformaciones de datos
- **IntegraciÃ³n**: Se conecta perfectamente con pandas DataFrames
- **Window functions**: Funciones como `LAG()` y `PARTITION BY` son nativas

**Conceptos SQL clave:**

- `LAG(columna, n)`: Obtiene el valor n perÃ­odos atrÃ¡s
- `PARTITION BY`: Agrupa los datos (por cliente en este caso)
- `ORDER BY`: Define el orden temporal (por foto_mes)

```python
# src/features.py
import pandas as pd
import duckdb
import logging

logger = logging.getLogger("__name__")

def feature_engineering_lag(df: pd.DataFrame, columnas: list[str], cant_lag: int = 1) -> pd.DataFrame:
    """
    Genera variables de lag para los atributos especificados utilizando SQL.
  
    Parameters:
    -----------
    df : pd.DataFrame
        DataFrame con los datos
    columnas : list
        Lista de atributos para los cuales generar lags. Si es None, no se generan lags.
    cant_lag : int, default=1
        Cantidad de lags a generar para cada atributo
  
    Returns:
    --------
    pd.DataFrame
        DataFrame con las variables de lag agregadas
    """

    logger.info(f"Realizando feature engineering con {cant_lag} lags para {len(columnas) if columnas else 0} atributos")

    if columnas is None or len(columnas) == 0:
        logger.warning("No se especificaron atributos para generar lags")
        return df
  
    # Construir la consulta SQL
    sql = "SELECT *"
  
    # Agregar los lags para los atributos especificados
    for attr in columnas:
        if attr in df.columns:
            for i in range(1, cant_lag + 1):
                sql += f", lag({attr}, {i}) OVER (PARTITION BY numero_de_cliente ORDER BY foto_mes) AS {attr}_lag_{i}"
        else:
            logger.warning(f"El atributo {attr} no existe en el DataFrame")
  
    # Completar la consulta
    sql += " FROM df"

    logger.debug(f"Consulta SQL: {sql}")

    # Ejecutar la consulta SQL
    con = duckdb.connect(database=":memory:")
    con.register("df", df)
    df = con.execute(sql).df()
    con.close()

    print(df.head())
  
    logger.info(f"Feature engineering completado. DataFrame resultante con {df.shape[1]} columnas")

    return df
```

---

## 7) `main.py` orquestando todo con logs

Esta es la **orquestaciÃ³n final** donde integramos todos los mÃ³dulos creados. El archivo `main.py` actÃºa como el **director de orquesta**, coordinando el flujo completo del procesamiento de datos.

**PatrÃ³n de orquestaciÃ³n:**

1. **ConfiguraciÃ³n inicial**: Setup de logging y directorios
2. **Carga de datos**: Usando el mÃ³dulo `loader.py`
3. **TransformaciÃ³n**: Aplicando feature engineering con `features.py`
4. **Persistencia**: Guardando los resultados procesados
5. **Logging completo**: Registrando cada paso del proceso

**Beneficios de este enfoque:**

- **Trazabilidad**: Cada operaciÃ³n queda registrada en logs
- **Modularidad**: Cada responsabilidad estÃ¡ en su mÃ³dulo correspondiente
- **Mantenibilidad**: FÃ¡cil modificar o extender funcionalidades
- **Debugging**: Los logs facilitan identificar problemas
- **Profesionalismo**: Estructura tÃ­pica de proyectos de producciÃ³n

```python
# main.py
import pandas as pd
import os
import datetime
import logging

from src.loader import cargar_datos
from src.features import feature_engineering_lag

## config basico logging
os.makedirs("logs", exist_ok=True)

fecha = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
monbre_log = f"log_{fecha}.log"
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(levelname)s - %(name)s %(lineno)d - %(message)s',
    handlers=[
        logging.FileHandler(f"logs/{monbre_log}", mode="w", encoding="utf-8"),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

## Funcion principal
def main():
    logger.info("Inicio de ejecucion.")

    #00 Cargar datos
    os.makedirs("data", exist_ok=True)
    path = "data/competencia_01.csv"
    df = cargar_datos(path)   

    #01 Feature Engineering
    atributos = ["ctrx_quarter"]
    cant_lag = 2
    df = feature_engineering_lag(df, columnas=atributos, cant_lag=cant_lag)
  
    #02 Guardar datos
    path = "data/competencia_01_lag.csv"
    df.to_csv(path, index=False)
  
    logger.info(f">>> EjecuciÃ³n finalizada. Revisar logs para mas detalles.{monbre_log}")

if __name__ == "__main__":
    main()
```

---

ğŸ“¦ **Dependencias necesarias**:

```bash
pip install pandas duckdb
```
